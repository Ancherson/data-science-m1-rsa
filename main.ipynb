{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient-Boosted Tree Classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler,StandardScaler\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import Imputer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/03/24 09:53:00 WARN Utils: Your hostname, MacBook-Pro-de-Matheo.local resolves to a loopback address: 127.0.0.1; using 10.10.24.145 instead (on interface en0)\n",
      "23/03/24 09:53:00 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Users/nebulo/opt/anaconda3/lib/python3.9/site-packages/pyspark/jars/spark-unsafe_2.12-3.0.3.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "23/03/24 09:53:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\").config(\"spark.driver.memory\", \"6g\").getOrCreate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load of train set and labels and merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "input_train = spark.read.csv(\"./InputTrain.csv\",header=True,inferSchema=True)\n",
    "label_train = spark.read.csv(\"./StepOne_LabelTrain.csv\",header=True,inferSchema=True)\n",
    "\n",
    "train_data = input_train.join(label_train, on=\"Index\",how=\"inner\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of the assembler and apply on the merged data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble the features into a vector\n",
    "assembler = VectorAssembler(inputCols=input_train.columns[:1], outputCol=\"features\")\n",
    "train_data = assembler.transform(train_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract randomly taken training set and a validation set from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and validation sets\n",
    "(train_set, validation_set) = train_data.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of the GBT and the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Precision Washing Machine = 0.6172365750771787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Precision Dishwasher = 0.6813958671125768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Precision Tumble Dryer = 0.9437826371546058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Precision Microwave = 0.6903534698715736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 768:=======>                                                 (1 + 7) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Precision Kettle = 0.8128432987587253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Train the GBT model\n",
    "models = []\n",
    "for i in range(0,5):\n",
    "    label_col = label_train.columns[2+i]\n",
    "    gbt = GBTClassifier(labelCol=label_col,featuresCol=\"features\",maxIter=10,maxDepth=5,maxBins=16)\n",
    "    model = gbt.fit(train_data)\n",
    "    \n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol=label_col,metricName=\"weightedPrecision\")\n",
    "    predictions = model.transform(validation_set)\n",
    "    weightedPrecision = evaluator.evaluate(predictions)\n",
    "    print(f\"Weighted Precision {label_col} = {weightedPrecision}\")\n",
    "    \n",
    "    models.append(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of the evaluator and compute of the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_missing_values(df):\n",
    "    \"\"\"Imputes missing values in the DataFrame\"\"\"\n",
    "    imputer = Imputer(inputCols=df.columns, outputCols=[\"{}_imputed\".format(c) for c in df.columns])\n",
    "    imputed_data = imputer.fit(df).transform(df)\n",
    "    return imputed_data\n",
    "\n",
    "def extract_features(df):\n",
    "    \"\"\"Extracts features from the DataFrame\"\"\"\n",
    "    feature_cols = [c for c in df.columns if \"imputed\" in c and \"index\" not in c and \"house_id\" not in c]\n",
    "    assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "    feature_data = assembler.transform(df)\n",
    "    return feature_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on `InputTest.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/03/24 10:11:17 WARN DAGScheduler: Broadcasting large task binary with size 1422.1 KiB\n",
      "23/03/24 10:11:25 WARN DAGScheduler: Broadcasting large task binary with size 2.6 MiB\n",
      "23/03/24 10:11:26 WARN DAGScheduler: Broadcasting large task binary with size 1048.0 KiB\n",
      "23/03/24 10:11:27 WARN DAGScheduler: Broadcasting large task binary with size 1048.0 KiB\n",
      "23/03/24 10:11:27 WARN DAGScheduler: Broadcasting large task binary with size 1048.0 KiB\n",
      "23/03/24 10:11:40 WARN DAGScheduler: Broadcasting large task binary with size 1507.3 KiB\n",
      "23/03/24 10:11:40 WARN DAGScheduler: Broadcasting large task binary with size 1517.4 KiB\n",
      "23/03/24 10:11:40 WARN DAGScheduler: Broadcasting large task binary with size 1515.6 KiB\n",
      "23/03/24 10:11:40 WARN DAGScheduler: Broadcasting large task binary with size 1508.0 KiB\n",
      "23/03/24 10:11:57 WARN DAGScheduler: Broadcasting large task binary with size 1351.1 KiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "test_data = spark.read.csv(\"InputTest.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Apply the same transformations used for the training set\n",
    "test_data_cleaned = test_data.drop('Index_Imputed')\n",
    "test_data_imputed = impute_missing_values(test_data_cleaned)\n",
    "test_data_features = extract_features(test_data_imputed)\n",
    "test_data_features = test_data_features.drop(\"features\")\n",
    "\n",
    "# Create a vector assembler to combine the features into a single column\n",
    "vector_assembler = VectorAssembler(inputCols=test_data_features.columns[2:], outputCol='features')\n",
    "test_data_with_features = vector_assembler.transform(test_data_features).select('Index', 'features')\n",
    "\n",
    "# apply the trained model on the test data and generate predictions\n",
    "test_predictions = {}\n",
    "for i in range(0,5):\n",
    "    label_col = label_train.columns[2+i]\n",
    "    model = models[i]\n",
    "    predictions = model.transform(test_data_with_features)\n",
    "    predictions = predictions.select('index', 'prediction')\n",
    "    test_predictions[label_col] = predictions.withColumnRenamed('prediction', label_col)\n",
    "\n",
    "# merge the individual predictions into a single DataFrame\n",
    "test_predictions_df = test_predictions[label_train.columns[2]]\n",
    "for i in range(1,5):\n",
    "    label_col = label_train.columns[2+i]\n",
    "    test_predictions_df = test_predictions_df.join(test_predictions[label_col], ['Index'], 'inner')\n",
    "\n",
    "# write the predictions to a CSV file\n",
    "test_predictions_df.toPandas().to_csv('StepOne_TestPredictions.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
